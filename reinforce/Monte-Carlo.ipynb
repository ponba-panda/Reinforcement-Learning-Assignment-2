{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F     \n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Param state is a torch tensor\n",
    "        \"\"\"\n",
    "        x = state\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.softmax(self.linear2(x), dim = -1)\n",
    "            \n",
    "        return x #Returns Probabilities of each action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic in Case of Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Param state is a torch tensor\n",
    "        \"\"\"\n",
    "        x = state\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCReinforceAgent:\n",
    "    def __init__(self, env:gym.Env, hidden_size=256, learning_rate = 0.01, gamma=0.99, baseline: Critic = None, toggle_target = False):\n",
    "        # Params\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.gamma = gamma\n",
    "        self.toggle_target = toggle_target\n",
    "        \n",
    "        # Policy Network\n",
    "        self.policy = Policy(self.num_states,hidden_size,self.num_actions).to(device)\n",
    "        \n",
    "        if baseline is not None:\n",
    "            self.baseline = True\n",
    "            self.critic_network = baseline.to(device)\n",
    "            self.critic_optimizer = optim.Adam(self.policy.parameters(), lr=self.critic_network.learning_rate)\n",
    "            self.critic_criterion = nn.MSELoss() \n",
    "            if toggle_target:\n",
    "                self.critic_target = copy.deepcopy(baseline).to(device)\n",
    "                self.tau = 0.01\n",
    "                for target_param, param in zip(self.critic_target.parameters(), self.critic_network.parameters()):\n",
    "                    target_param.data.copy_(param.data)\n",
    "                    target_param.requires_grad = False\n",
    "            else:\n",
    "                self.critic_td_losses = [[],[]]\n",
    "        else:\n",
    "            self.baseline = False\n",
    "\n",
    "        # Action & Reward Buffer (For a given episode)\n",
    "        self.saved_actions = [] #Elements are NamedTuples\n",
    "        self.rewards = []\n",
    "\n",
    "        # Training\n",
    "        self.optimizer  = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        probs = self.policy(state)\n",
    "\n",
    "        if self.baseline:\n",
    "            state_value = self.critic_network(state)\n",
    "        else:\n",
    "            state_value = None\n",
    "\n",
    "        # create a categorical distribution over the list of probabilities of actions\n",
    "        m = Categorical(probs)\n",
    "\n",
    "        # and sample an action using the distribution\n",
    "        action = m.sample()\n",
    "\n",
    "        # save to action buffer\n",
    "        self.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "\n",
    "        # the action to take\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, returns):\n",
    "        \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps) \n",
    "        T = 1\n",
    "        policy_losses = []\n",
    "        for (log_prob, value), R in zip(self.saved_actions, returns):\n",
    "            if self.baseline == False:\n",
    "                advantage = R \n",
    "            else:\n",
    "                advantage = R - value.item()\n",
    "\n",
    "            # calculate policy loss\n",
    "            policy_losses.append(-log_prob * advantage/T)\n",
    "\n",
    "        # reset gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # sum up all the values of policy_losses\n",
    "        loss = torch.stack(policy_losses).sum()\n",
    "\n",
    "        # perform backprop\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.baseline and self.toggle_target == False:\n",
    "            # reset gradients\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            loss = self.critic_criterion(self.critic_td_losses[0][0],self.critic_td_losses[1][0])\n",
    "\n",
    "            # perform backprop\n",
    "            loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "            self.critic_td_losses = [[],[]]\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def update_critic(self,state,reward,next_state,done):\n",
    "        state = torch.tensor(state).to(device)\n",
    "        next_state = torch.tensor(next_state).to(device)\n",
    "        reward = torch.tensor([reward]).to(device)\n",
    "        if done:\n",
    "            target = torch.tensor([0]).to(device)\n",
    "        else:\n",
    "            if self.toggle_target:\n",
    "                target = self.critic_target(next_state).to(device)\n",
    "            else:\n",
    "                target = self.critic_network(next_state).to(device)\n",
    "        if not self.toggle_target:\n",
    "            self.critic_td_losses[0].append(reward + self.gamma*target)\n",
    "            self.critic_td_losses[1].append(reward + self.critic_network(state))\n",
    "        else:\n",
    "            loss = self.critic_criterion(reward + self.gamma*target, self.critic_network(state))\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "            for target_param, param in zip(self.critic_target.parameters(), self.critic_network.parameters()):\n",
    "                target_param.data.copy_(self.tau*param.data + (1-self.tau)*target_param.data)\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env: gym.Env, agent: MCReinforceAgent, running_reward_init, num_episodes, log_interval=10, terminate_on_threshold=False):\n",
    "    '''\n",
    "    Returns episode wise returns\n",
    "    '''\n",
    "    running_reward = running_reward_init\n",
    "    returns = []\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # reset environment and episode reward\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        for t in range(1, 10000):\n",
    "\n",
    "            # select action from policy\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # take the action\n",
    "            next_state, reward, done1, done2, _ = env.step(action)\n",
    "            done = done1 or done2\n",
    "            if agent.baseline:\n",
    "                agent.update_critic(state, reward, next_state, done)\n",
    "            agent.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        # update cumulative reward\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate Returns\n",
    "        R = 0\n",
    "        returns_for_agent = []\n",
    "        for r in agent.rewards[::-1]:\n",
    "            # calculate the discounted value\n",
    "            R = r + agent.gamma * R\n",
    "            returns_for_agent.insert(0, R)\n",
    "        returns.append(R)\n",
    "        agent.update(returns_for_agent)\n",
    "\n",
    "        # log results\n",
    "        if log_interval is not None:\n",
    "            if i_episode % log_interval == 0:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                    i_episode, ep_reward, running_reward))\n",
    "\n",
    "        # check if we have \"solved\" the cart pole problem\n",
    "        if (terminate_on_threshold and running_reward > env.spec.reward_threshold):\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "    return returns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Plot\n",
    "\n",
    "5 random seeds + plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test5(env: gym.Env, agent_params: np.ndarray, critic_params: np.ndarray, running_reward_init, num_episodes, log_interval=10, terminate_on_threshold=False, plot=True, savefile=None, toggle_target = False):\n",
    "    ''' \n",
    "    agent_params = (hidden_size, learning_rate)\n",
    "    critic_params = (None) if not critic, (hidden_size, learning_rate) otherwise\n",
    "    '''\n",
    "    agent_lrs = []\n",
    "    agent_hls = []\n",
    "    critic_lrs = []\n",
    "    critic_hls = []\n",
    "    avg_regrets = []\n",
    "    averaged_returns = []\n",
    "    returns_all_params = []\n",
    "    for i in range(len(agent_params)):\n",
    "        print(f\"Parameter Set {i+1}:\")\n",
    "        agent_hidden_size = agent_params[i][0]\n",
    "        agent_learning_rate = agent_params[i][1]\n",
    "        agent_lrs.append(agent_learning_rate)\n",
    "        agent_hls.append(agent_hidden_size)\n",
    "        if critic_params[i] is not None:\n",
    "            critic_hidden_size = critic_params[i][0]\n",
    "            critic_learning_rate = critic_params[i][1]\n",
    "            critic_lrs.append(critic_learning_rate)\n",
    "            critic_hls.append(critic_hidden_size)\n",
    "        else:\n",
    "            critic_lrs.append(np.nan)\n",
    "            critic_hls.append(np.nan)\n",
    "        returns = []\n",
    "        for j in range(5):\n",
    "            seed = int(np.random.randint(low=1, high=1034300))\n",
    "            env.reset(seed=seed)\n",
    "            torch.manual_seed(seed)\n",
    "            if critic_params[i] is not None:\n",
    "                critic = Critic(\n",
    "                    env.observation_space.shape[0], hidden_size=critic_hidden_size, output_size=1, learning_rate=critic_learning_rate)\n",
    "            else:\n",
    "                critic = None\n",
    "            agent = MCReinforceAgent(env, hidden_size=agent_hidden_size,\n",
    "                                     learning_rate=agent_learning_rate, gamma=0.99, baseline=critic, toggle_target=toggle_target)\n",
    "            a = train(env, agent, running_reward_init, num_episodes,\n",
    "                                 log_interval, terminate_on_threshold)\n",
    "            returns.append(a)\n",
    "            \n",
    "        returns_all_params.append(returns)\n",
    "        averaged_returns.append(np.mean(np.array(returns), axis=0))\n",
    "        if env.spec.id == 'Acrobot-v1':\n",
    "            avg_regrets.append(np.sum(\n",
    "                (1-(0.99)**100)/0.01*-np.ones(len(returns[0])) - (np.mean(np.array(returns), axis=0))))\n",
    "        elif env.spec.id == 'CartPole-v1':\n",
    "            avg_regrets.append(np.sum(\n",
    "                (1-(0.99)**500)/0.01*np.ones(len(returns[0])) - (np.mean(np.array(returns), axis=0))))\n",
    "    if savefile is not None:\n",
    "        try:\n",
    "            df = pd.DataFrame()\n",
    "            df[\"agent_lr\"] = agent_lrs\n",
    "            df[\"agent_hl\"] = agent_hls\n",
    "            df[\"critic_lr\"] = critic_lrs\n",
    "            df[\"critic_hl\"] = critic_hls\n",
    "            df[\"avg_regrets\"] = avg_regrets\n",
    "\n",
    "            df.to_csv(savefile)\n",
    "        except Exception as e:\n",
    "            print(f\"Error Saving!{e}\")\n",
    "\n",
    "    if plot:\n",
    "        plot_returns(returns_all_params, title=\"Acrobot-v1 with REINFORCE\", labels=['no_base','base'])\n",
    "\n",
    "    return returns_all_params\n",
    "\n",
    "\n",
    "def plot_returns(returns_all_params,labels,title:str = None):\n",
    "    \n",
    "    x = np.arange(1, len(returns_all_params[0][0])+1)\n",
    "    plt.figure()\n",
    "    for i in range(len(returns_all_params)):\n",
    "        returns = np.array(returns_all_params[i])\n",
    "        avg_returns = np.mean(returns, axis=0)\n",
    "        std_returns = np.std(returns, axis=0)\n",
    "        plt.plot(x, avg_returns, label = labels[i])\n",
    "        plt.fill_between(x, avg_returns-std_returns,\n",
    "                         avg_returns+std_returns, alpha=0.5, label = None)\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average return per episode')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Without baseline\n",
    "policy_params =[[64,3e-2],[64,3e-3],[64,3e-4]]\n",
    "no_critic_params = [None for i in policy_params]\n",
    "\n",
    "#With baseline\n",
    "critic_params_ = [[32,3e-2],[32,3e-3],[32,3e-4],\\\n",
    "                  [64,3e-2],[64,3e-2],[64,3e-2]]\n",
    "critic_params = []\n",
    "for i in critic_params_:\n",
    "    critic_params += [i]*len(policy_params)\n",
    "policy_params_baseline = policy_params*len(critic_params_)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "test5(env, policy_params,\\\n",
    "            no_critic_params,\\\n",
    "                  0, 1000, log_interval = 100,savefile='cp_wb.csv', plot= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "test5(env, policy_params,\\\n",
    "            no_critic_params,\\\n",
    "                  -500, 2000, log_interval = 100,savefile='ac_wb.csv', plot= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "test5(env, policy_params_baseline,\\\n",
    "            critic_params,\\\n",
    "                  0, 2000, log_interval = 100,savefile='cp_b.csv', plot= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "test5(env, policy_params_baseline,\\\n",
    "            critic_params,\\\n",
    "                  -500, 2000, log_interval = 100,savefile='ac_b.csv', plot= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Obtained Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "returns_final_cart = test5(env, [[64, 3e-3],[64, 3e-3]],\n",
    "                       [[32,3e-4],None],\n",
    "                       -500, 1000, log_interval=10, savefile=None, plot=True, toggle_target=False) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
